<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.1 Discrete random variables: An example using the Binomial distribution | Linear Mixed Models in Linguistics and Psychology: A Comprehensive Introduction</title>
  <meta name="description" content="Linear Mixed Models for Linguistics and Psychology: A Comprehensive Introduction" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="1.1 Discrete random variables: An example using the Binomial distribution | Linear Mixed Models in Linguistics and Psychology: A Comprehensive Introduction" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://bookdown.org/yihui/bookdown/" />
  <meta property="og:image" content="https://bookdown.org/yihui/bookdown/images/temporarycover.jpg" />
  <meta property="og:description" content="Linear Mixed Models for Linguistics and Psychology: A Comprehensive Introduction" />
  <meta name="github-repo" content="rstudio/bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.1 Discrete random variables: An example using the Binomial distribution | Linear Mixed Models in Linguistics and Psychology: A Comprehensive Introduction" />
  
  <meta name="twitter:description" content="Linear Mixed Models for Linguistics and Psychology: A Comprehensive Introduction" />
  <meta name="twitter:image" content="https://bookdown.org/yihui/bookdown/images/temporarycover.jpg" />

<meta name="author" content="Shravan Vasishth, Daniel Schad, Audrey BÃ¼rki, Reinhold Kliegl" />


<meta name="date" content="2020-07-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="continuous-random-variables-an-example-using-the-normal-distribution.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Authoring Books with R Markdown</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.2</b> How to read this book</a></li>
<li class="chapter" data-level="0.3" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.3</b> Online materials</a></li>
<li class="chapter" data-level="0.4" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.4</b> Software needed</a></li>
<li class="chapter" data-level="0.5" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>0.5</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html"><i class="fa fa-check"></i><b>1.1</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.1.1" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.1.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.1.2" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.1.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.2</b> Continuous random variables: An example using the Normal distribution</a></li>
<li class="chapter" data-level="1.3" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.3</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.4" data-path="summary-of-random-variable-theory.html"><a href="summary-of-random-variable-theory.html"><i class="fa fa-check"></i><b>1.4</b> *Summary of random variable theory</a></li>
<li class="chapter" data-level="1.5" data-path="summary-of-concepts-introduced-in-this-chapter.html"><a href="summary-of-concepts-introduced-in-this-chapter.html"><i class="fa fa-check"></i><b>1.5</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="1.6" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.6</b> Further reading</a></li>
<li class="chapter" data-level="1.7" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.7</b> Exercises</a><ul>
<li class="chapter" data-level="1.7.1" data-path="exercises.html"><a href="exercises.html#practice-using-the-pnorm-function"><i class="fa fa-check"></i><b>1.7.1</b> Practice using the <code>pnorm</code> function</a></li>
<li class="chapter" data-level="1.7.2" data-path="exercises.html"><a href="exercises.html#practice-using-the-qnorm-function"><i class="fa fa-check"></i><b>1.7.2</b> Practice using the <code>qnorm</code> function</a></li>
<li class="chapter" data-level="1.7.3" data-path="exercises.html"><a href="exercises.html#practice-using-qt"><i class="fa fa-check"></i><b>1.7.3</b> Practice using <code>qt</code></a></li>
<li class="chapter" data-level="1.7.4" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.7.4</b> Maximum likelihood estimation 1</a></li>
<li class="chapter" data-level="1.7.5" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>1.7.5</b> Maximum likelihood estimation 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introBDA.html"><a href="introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.1</b> Deriving the posterior using Bayesâ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.1.2" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-prior-for-theta"><i class="fa fa-check"></i><b>2.1.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.1.3</b> Using Bayesâ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.1.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.1.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.1.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.1.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.1.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.1.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.1.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="summary-of-concepts-introduced-in-this-chapter-1.html"><a href="summary-of-concepts-introduced-in-this-chapter-1.html"><i class="fa fa-check"></i><b>2.2</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="2.3" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="2.4.1" data-path="exercises-1.html"><a href="exercises-1.html#deriving-bayes-rule"><i class="fa fa-check"></i><b>2.4.1</b> Deriving Bayesâ rule</a></li>
<li class="chapter" data-level="2.4.2" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-1"><i class="fa fa-check"></i><b>2.4.2</b> Conjugate forms 1</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-2"><i class="fa fa-check"></i><b>2.4.3</b> Conjugate forms 2</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-3"><i class="fa fa-check"></i><b>2.4.4</b> Conjugate forms 3</a></li>
<li class="chapter" data-level="2.4.5" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-4"><i class="fa fa-check"></i><b>2.4.5</b> Conjugate forms 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="important-distributions.html"><a href="important-distributions.html"><i class="fa fa-check"></i><b>3</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Linear Mixed Models in Linguistics and Psychology: A Comprehensive Introduction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="discrete-random-variables-an-example-using-the-binomial-distribution" class="section level2">
<h2><span class="header-section-number">1.1</span> Discrete random variables: An example using the Binomial distribution</h2>
<p>Imagine that our data come from a grammaticality judgement task, and that the responses from participants are a sequence of 1âs and 0âs, where 1 represents the judgment âgrammaticalâ, and 0 represents the judgement âungrammaticalâ. Assume also that each response, coded as 1 or 0, is generated independently from the others. We can simulate such a sequence of 1s and 0s in R. Here is a case where we run the same experiment <span class="math inline">\(20\)</span> times (the sample size is <span class="math inline">\(10\)</span> each time).</p>
<pre><code>##  [1] 7 7 4 7 6 5 6 3 6 6 5 6 7 4 5 7 8 3 5 5</code></pre>
<p>The number of successes in each of the <span class="math inline">\(20\)</span> simulated experiments above is being generated by a <em>discrete random variable</em> <span class="math inline">\(Y\)</span> with a probability distribution <span class="math inline">\(p(Y)\)</span> called the <strong>Binomial distribution</strong>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>For discrete random variable, the probability distribution <span class="math inline">\(p(y)\)</span> is called a <strong>probability mass function</strong> (PMF). The PMF defines the probability of each possible outcome. In the above example, with <span class="math inline">\(n=10\)</span> trials, there are 11 possible outcomes: <span class="math inline">\(0,\dots,10\)</span> successes. Which of these outcomes is most probable depends on a parameter in the Binomial distribution that represents the probability of success. We will call this parameter <span class="math inline">\(\theta\)</span>. The left-hand side plot in Figure <a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#fig:binomplot">1.1</a> shows an example of a Binomial PMF with <span class="math inline">\(10\)</span> trials and the parameter <span class="math inline">\(\theta\)</span> with value <span class="math inline">\(0.5\)</span>. Setting <span class="math inline">\(\theta\)</span> to 0.5 leads to a PMF where the most probable outcome is 5 successes out of 10. If we had set <span class="math inline">\(\theta\)</span> to, say 0.1, then the most probable outcome would be 1 success out of 10; and if we had set <span class="math inline">\(\theta\)</span> to 0.9, then the most probable outcome would be 9 successes out of 10.</p>
<div class="figure"><span id="fig:binomplot"></span>
<img src="bookdown_files/figure-html/binomplot-1.svg" alt="Probability mass functions of a binomial distribution assuming 10 trials, with 50%, 10%, and 90% probability of success." width="672" />
<p class="caption">
FIGURE 1.1: Probability mass functions of a binomial distribution assuming 10 trials, with 50%, 10%, and 90% probability of success.
</p>
</div>

<div class="rmdnote">
to-do bar or line graphs above, instead of points
</div>

<p>The probability mass function for the binomial is written as follows.</p>
<p><span class="math display">\[\begin{equation}
\hbox{Binomial}(k|n,\theta) = 
\binom{n}{k} \theta^{k} (1-\theta)^{n-k}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(n\)</span> represents the total number of trials, <span class="math inline">\(k\)</span> the number of successes, and <span class="math inline">\(\theta\)</span> the probability of success. The term <span class="math inline">\(\binom{n}{k}\)</span>, pronounced n-choose-k, represents the number of ways in which one can choose <span class="math inline">\(k\)</span> successes out of <span class="math inline">\(n\)</span> trials. For example, 1 success out of 10 can occur in 10 possible ways: the very first trial could be a 1, the secone trial could be a 1, etc.
The term <span class="math inline">\(\binom{n}{k}\)</span> expands to <span class="math inline">\(\frac{n!}{k!(n-k)!}\)</span>. In <code>R</code>, it is computed using the function <code>choose(n,k)</code>, with <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span> representing positive integer values.</p>
<div id="the-mean-and-variance-of-the-binomial-distribution" class="section level3">
<h3><span class="header-section-number">1.1.1</span> The mean and variance of the Binomial distribution</h3>
<p>It is possible to analytically compute the mean and variance of the PMF associated with the Binomial random variable <span class="math inline">\(Y\)</span>. Without getting into the details of how these are derived mathematically, we just state here that the mean of <span class="math inline">\(Y\)</span> (also called the expectation, conventionally written <span class="math inline">\(E[Y]\)</span>) and variance of <span class="math inline">\(Y\)</span> (written <span class="math inline">\(Var(Y)\)</span>) of a Binomial distribution with parameter <span class="math inline">\(\theta\)</span> and <span class="math inline">\(n\)</span> trials are <span class="math inline">\(E[Y] = n\theta\)</span> and <span class="math inline">\(Var(Y) = n\theta (1-\theta)\)</span>, respectively.</p>
<p>Of course, we always know <span class="math inline">\(n\)</span> (because we decide on the number of trials ourselves), but in real experimental situations we never know the true value of <span class="math inline">\(\theta\)</span>. But <span class="math inline">\(\theta\)</span> can be estimated from the data. From the observed data, we can compute the estimate of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\hat \theta=k/n\)</span>. The quantity <span class="math inline">\(\hat \theta\)</span> is the observed proportion of successes, and is called the <strong>maximum likelihood estimate</strong> of the true (but unknown mean). Once we have estimated <span class="math inline">\(\theta\)</span> in this way, we can also obtain an estimate (also a maximum likelihood estimate) of the variance by computing <span class="math inline">\(n\\hattheta (1-\hat\theta)\)</span>. These estimates are then used for statistical inference.</p>
<p>What does the term âmaximum likelihood estimateâ mean? The term <strong>likelihood</strong> refers to the value of the Binomial distribution function for a particular value of <span class="math inline">\(\theta\)</span>, once we have observed some data. For example, suppose you record <span class="math inline">\(n=10\)</span> trials, and observe <span class="math inline">\(k=7\)</span> successes. What is the probability of observing <span class="math inline">\(7\)</span> successes out of <span class="math inline">\(10\)</span>? We need the binomial distribution to compute this value:</p>
<p><span class="math display">\[\begin{equation}
\hbox{Binomial}(k=7|n=10,\theta) = 
\binom{10}{7} \theta^{7} (1-\theta)^{10-7}
\end{equation}\]</span></p>
<p>Once we have observed the data, both <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span> are fixed. The only variable in the above equation now is <span class="math inline">\(\theta\)</span>: the above function is now only dependent on the value of <span class="math inline">\(\theta\)</span>. When the data are fixed, the probability mass function is only dependent on the value of the parameter <span class="math inline">\(\theta\)</span>, and is called a <strong>likelihood function</strong>. It is therefore often expressed as a function of <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math inline">\(p( y | theta ) = p( k=7, n=10 | theta) = \mathcal{L}(\theta)\)</span></p>
<p>The vertical bar notation above should be read as saying that, given some data <span class="math inline">\(y\)</span> (which in the binomial case will be <span class="math inline">\(k\)</span> âsuccessesâ in <span class="math inline">\(n\)</span> trials), the function returns a value for different values of <span class="math inline">\(\theta\)</span>.</p>
<p>If we now plot this function for all possible values of <span class="math inline">\(\theta\)</span>, we get the plot shown in Figure <a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#fig:binomlik">1.2</a>.</p>
<div class="figure"><span id="fig:binomlik"></span>
<img src="bookdown_files/figure-html/binomlik-1.svg" alt="The likelihood function for 7 successes out of 10." width="672" />
<p class="caption">
FIGURE 1.2: The likelihood function for 7 successes out of 10.
</p>
</div>

<div class="rmdnote">
DS comment: do we want to show the code for computing all likelihood values? (maybe this comes later?)
</div>

<p>What is important about this plot is that it shows that, given the data, the maximum point is at the point <span class="math inline">\(0.7\)</span>, which corresponds to the estimated mean using the formula shown above: <span class="math inline">\(k/n = 7/10\)</span>. Thus, the maximum likelihood estimate (MLE) gives us the most likely value that the parameter <span class="math inline">\(\theta\)</span> given the data. It is crucial to note here that the phrase âmost likelyâ here does not mean that the MLE from a <em>particular</em> sample of data invariably gives us an accurate estimate of <span class="math inline">\(\theta\)</span>. For example, if we run our experiment for <span class="math inline">\(10\)</span> trials and get <span class="math inline">\(1\)</span> success out of <span class="math inline">\(10\)</span>, the MLE is <span class="math inline">\(0.10\)</span>. We could have happened to observe only one success out of ten even if the true <span class="math inline">\(\theta\)</span> were <span class="math inline">\(0.5\)</span>. The MLE would however give an accurate estimate of the true parameter as <span class="math inline">\(n\)</span> approaches infinity.</p>
</div>
<div id="what-information-does-a-probability-distribution-provide" class="section level3">
<h3><span class="header-section-number">1.1.2</span> What information does a probability distribution provide?</h3>
<p>What good is a probability mass function? We consider this question next.</p>
<div id="compute-the-probability-of-a-particular-outcome-discrete-case-only" class="section level4">
<h4><span class="header-section-number">1.1.2.1</span> Compute the probability of a particular outcome (discrete case only)</h4>
<p>The Binomial distribution shown in Figure <a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#fig:binomplot">1.1</a> already shows the probability of each possible outcome under a different value for <span class="math inline">\(\theta\)</span>. In R, there is a built-in function that allows us to calculate the probability of <span class="math inline">\(k\)</span> successes out of <span class="math inline">\(n\)</span>, given a particular value of <span class="math inline">\(k\)</span> (this number constitutes our data), the number of trials <span class="math inline">\(n\)</span>, and given a particular value of <span class="math inline">\(\theta\)</span>; this is the <code>dbinom</code> function. For example, the probability of 5 successes out of 10 when <span class="math inline">\(\theta\)</span> is 0.5 is:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">dbinom</span>(<span class="dv">5</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)</a></code></pre></div>
<pre><code>## [1] 0.2461</code></pre>
<p>The probabilities of success when <span class="math inline">\(\theta\)</span> is 0.1 or 0.9 can be computed by replacing 0.5 above by each of these probabilities. One can just do this by giving <code>dbinom</code> a vector of probabilities:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="kw">dbinom</span>(<span class="dv">5</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="kw">c</span>(<span class="fl">0.1</span>,<span class="fl">0.9</span>))</a></code></pre></div>
<pre><code>## [1] 0.001488 0.001488</code></pre>
<p>Note that the probability of a particular outcome is only computable in the discrete case; in the continuous case, this probability will always be zero (we discuss this in the next section).</p>
</div>
<div id="compute-the-cumulative-probability-of-k-or-less-more-than-k-successes" class="section level4">
<h4><span class="header-section-number">1.1.2.2</span> Compute the cumulative probability of k or less (more) than k successes</h4>
<p>Using the <code>dbinom</code> function, we can compute the cumulative probability of obtaining 1 or less, 2 or less successes etc. This is done through a simple summation procedure:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="co">## the cumulative probability of obtaining</span></a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="co">## 0, 1, or 2 successes out of 10,</span></a>
<a class="sourceLine" id="cb7-3" data-line-number="3"><span class="co">## with theta=0.5:</span></a>
<a class="sourceLine" id="cb7-4" data-line-number="4"><span class="kw">dbinom</span>(<span class="dv">0</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)<span class="op">+</span><span class="kw">dbinom</span>(<span class="dv">1</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb7-5" data-line-number="5"><span class="st">  </span><span class="kw">dbinom</span>(<span class="dv">2</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)</a></code></pre></div>
<pre><code>## [1] 0.05469</code></pre>
<p>Mathematically, we could write the above summation as:</p>
<p><span class="math display">\[\begin{equation}
\sum_{k=0}^2 \binom{n}{k} \theta^{k} (1-\theta)^{n-k} 
\end{equation}\]</span></p>
<p>An alternative to the cumbersome addition in the R code above is this more compact statement, which closely mimics the above mathematical expression:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="kw">sum</span>(<span class="kw">dbinom</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">2</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>))</a></code></pre></div>
<pre><code>## [1] 0.05469</code></pre>
<p>R has a built-in function called <code>pbinom</code> that does this summation for us. If we want to know the probability of <span class="math inline">\(2\)</span> or less successes as in the above example, we can write:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">pbinom</span>(<span class="dv">2</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>,<span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] 0.05469</code></pre>
<p>The specification <code>lower.tail=TRUE</code> ensures that the summation goes from <span class="math inline">\(2\)</span> to numbers smaller than <span class="math inline">\(2\)</span> (which lie in the lower tail of the distribution in Figure <a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#fig:binomplot">1.1</a>). If we wanted to know what the probability is of obtaining <span class="math inline">\(2\)</span> or more successes out of <span class="math inline">\(10\)</span>, we can set <code>lower.tail</code> to <code>FALSE</code>:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="kw">pbinom</span>(<span class="dv">2</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>,<span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>## [1] 0.9453</code></pre>
<p>The cumulative distribution function or CDF can be plotted by computing the cumulative probabilities for any value <span class="math inline">\(k\)</span> or less than <span class="math inline">\(k\)</span>, where <span class="math inline">\(k\)</span> ranges from <span class="math inline">\(0\)</span> to <span class="math inline">\(10\)</span> in our running example. The CDF is shown in Figure <a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#fig:binomcdf">1.3</a>.</p>
<div class="figure"><span id="fig:binomcdf"></span>
<img src="bookdown_files/figure-html/binomcdf-1.svg" alt="The cumulative distribution function for a binomial distribution assuming 10 trials, with 50% probability of success." width="672" />
<p class="caption">
FIGURE 1.3: The cumulative distribution function for a binomial distribution assuming 10 trials, with 50% probability of success.
</p>
</div>
</div>
<div id="compute-the-inverse-of-the-cumulative-distribution-function-the-quantile-function" class="section level4">
<h4><span class="header-section-number">1.1.2.3</span> Compute the inverse of the cumulative distribution function (the quantile function)</h4>
<p>We can also find out the value of the variable <span class="math inline">\(k\)</span> (the quantile) such that the probability of obtaining <span class="math inline">\(k\)</span> or less than <span class="math inline">\(k\)</span> successes is some specific probability value <span class="math inline">\(p\)</span>. If we switch the x and y axes of Figure <a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#fig:binomcdf">1.3</a>, we obtain another very useful function, the inverse CDF.</p>
<p>The inverse of the CDF (known as the quantile function in R because it returns the quantile, the value k) is available in R as the function <code>qbinom</code>. The usage is as follows: to find out what the value <span class="math inline">\(k\)</span> of the outcome is such that the probability of obtaining <span class="math inline">\(k\)</span> or less successes is <span class="math inline">\(0.37\)</span>, type:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="kw">qbinom</span>(<span class="fl">0.37</span>,<span class="dt">size=</span><span class="dv">10</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)</a></code></pre></div>
<pre><code>## [1] 4</code></pre>
</div>
<div id="generate-random-data-from-a-hboxbinomialntheta-distribution" class="section level4">
<h4><span class="header-section-number">1.1.2.4</span> Generate random data from a <span class="math inline">\(\hbox{Binomial}(n,\theta)\)</span> distribution</h4>
<p>We can generate random simulated data from a Binomial distribution by specifying the number of trials and the probability of success <span class="math inline">\(\theta\)</span>. In <code>R</code>, we do this as follows:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="kw">rbinom</span>(<span class="dv">10</span>,<span class="dt">size=</span><span class="dv">1</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)</a></code></pre></div>
<pre><code>##  [1] 1 0 1 1 0 1 0 1 0 1</code></pre>
<p>The above code generates a sequences of <span class="math inline">\(1\)</span>âs and <span class="math inline">\(0\)</span>âs. Repeatedly run the above code; you will get different sequences each time. For each generated sequence, one can calculate the number of successes by just summing up the vector, or computing its mean and multiplying by the number of trials, here <span class="math inline">\(10\)</span>:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1">y&lt;-<span class="kw">rbinom</span>(<span class="dv">10</span>,<span class="dt">size=</span><span class="dv">1</span>,<span class="dt">prob=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb19-2" data-line-number="2"><span class="kw">mean</span>(y)<span class="op">*</span><span class="dv">10</span> ; <span class="kw">sum</span>(y)</a></code></pre></div>
<pre><code>## [1] 6</code></pre>
<pre><code>## [1] 6</code></pre>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>When an experiment consists of only a single trial (i.e., we can have a total number of only 0 or 1 successes), <span class="math inline">\(p(Y)\)</span> is called a <strong>Bernoulli distribution</strong>.<a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#fnref1" class="footnote-back">â©</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="continuous-random-variables-an-example-using-the-normal-distribution.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/01-Foundations.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
